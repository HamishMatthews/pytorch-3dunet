{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import tifffile\n",
    "\n",
    "data_dir = r\"C:\\Users\\hamis\\OneDrive\\Documents\\GitHub\\Vasculature-3DUnet\\sample_data\"\n",
    "\n",
    "def create_hdf5_dataset(data_dir, dataset_name):\n",
    "    # Paths to images and labels\n",
    "    image_dir = os.path.join(data_dir, 'train', dataset_name, 'images')\n",
    "    label_dir = os.path.join(data_dir, 'train', dataset_name, 'labels')\n",
    "\n",
    "    # Get sorted list of image and label paths\n",
    "    image_paths = sorted(glob(os.path.join(image_dir, '*.tif')))\n",
    "    label_paths = sorted(glob(os.path.join(label_dir, '*.tif')))\n",
    "\n",
    "    # Read the first image to get dimensions\n",
    "    sample_image = tifffile.imread(image_paths[0])\n",
    "    Z, Y, X = len(image_paths), sample_image.shape[0], sample_image.shape[1]\n",
    "\n",
    "    # get the data type of the tif\n",
    "    print(sample_image.dtype)\n",
    "\n",
    "    print(Z, Y, X)\n",
    "\n",
    "    # Create HDF5 file in the same directory as the notebook\n",
    "    hdf5_path = os.path.join(os.getcwd(), f'{dataset_name}.hdf5')\n",
    "    with h5py.File(hdf5_path, 'w') as hdf5_file:\n",
    "        # Create datasets for raw and label\n",
    "        hdf5_file.create_dataset('raw', (Z, Y, X), np.uint16)\n",
    "        hdf5_file.create_dataset('label', (Z, Y, X), np.uint16)\n",
    "\n",
    "        # Load images and labels into the HDF5 file\n",
    "        for i, (img_path, label_path) in enumerate(zip(image_paths, label_paths)):\n",
    "            img = tifffile.imread(img_path)\n",
    "            label = tifffile.imread(label_path)\n",
    "            print(f\"Image {img_path} - Min: {img.min()}, Max: {img.max()}\")\n",
    "            \n",
    "            hdf5_file['raw'][i, ...] = img\n",
    "            hdf5_file['label'][i, ...] = label\n",
    "\n",
    "    print(f'HDF5 file created for {dataset_name}: {hdf5_path}')\n",
    "\n",
    "# Create HDF5 for each dataset\n",
    "datasets = ['kidney_1_dense']\n",
    "for dataset in datasets:\n",
    "    create_hdf5_dataset(data_dir, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to read the 500th image from the hdf5 file\n",
    "from PIL import Image\n",
    "\n",
    "file = 'kidney_1_dense.hdf5'\n",
    "\n",
    "with h5py.File(file, 'r') as f:\n",
    "    print(f.keys())\n",
    "    print(f['raw'].shape)\n",
    "    print(f['raw'][500, ...].shape)\n",
    "    print(f['raw'][500, ...].dtype)\n",
    "    print(f['raw'][500, ...].min())\n",
    "    print(f['raw'][500, ...].max())\n",
    "\n",
    "    print(f['label'].shape)\n",
    "    print(f['label'][500, ...].shape)\n",
    "    print(f['label'][500, ...].dtype)\n",
    "    print(f['label'][500, ...].min())\n",
    "    print(f['label'][500, ...].max())\n",
    "\n",
    "    img = Image.fromarray(f['label'][501, ...])\n",
    "    img.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "original_hdf5_path = './kidney_1_dense.hdf5'  # Path to the original HDF5 file\n",
    "train_hdf5_path = './kidney_1_dense_train.hdf5'  # Path for the training HDF5 file\n",
    "val_hdf5_path = './kidney_1_dense_val.hdf5'  # Path for the validation HDF5 file\n",
    "\n",
    "# Open the original HDF5 file\n",
    "with h5py.File(original_hdf5_path, 'r') as original_file:\n",
    "    # Read the 'raw' and 'label' datasets\n",
    "    raw_dataset = original_file['raw']\n",
    "    label_dataset = original_file['label']\n",
    "\n",
    "    # Calculate the number of slices\n",
    "    num_slices = raw_dataset.shape[0]\n",
    "\n",
    "    print(raw_dataset.shape[0], raw_dataset.shape[1], raw_dataset.shape[2])\n",
    "\n",
    "    # Calculate the number of validation slices\n",
    "    num_val_slices = num_slices // 10 + (1 if num_slices % 10 != 0 else 0)\n",
    "\n",
    "    # Create new HDF5 files for training and validation\n",
    "    with h5py.File(train_hdf5_path, 'w') as train_file, h5py.File(val_hdf5_path, 'w') as val_file:\n",
    "        # Create datasets in the new HDF5 files\n",
    "        train_file.create_dataset('raw', (num_slices - num_val_slices, raw_dataset.shape[1], raw_dataset.shape[2]), np.uint16)\n",
    "        train_file.create_dataset('label', (num_slices - num_val_slices, label_dataset.shape[1], label_dataset.shape[2]), np.uint16)\n",
    "        val_file.create_dataset('raw', (num_val_slices, raw_dataset.shape[1], raw_dataset.shape[2]), np.uint16)\n",
    "        val_file.create_dataset('label', (num_val_slices, label_dataset.shape[1], label_dataset.shape[2]), np.uint16)\n",
    "\n",
    "        # Indices for training and validation datasets\n",
    "        train_idx, val_idx = 0, 0\n",
    "\n",
    "        # Iterate over the slices and distribute them into training and validation sets\n",
    "        for i in range(num_slices):\n",
    "            if i % 10 == 0:\n",
    "                # Add to validation set\n",
    "                val_file['raw'][val_idx, ...] = raw_dataset[i, ...]\n",
    "                val_file['label'][val_idx, ...] = label_dataset[i, ...]\n",
    "                val_idx += 1\n",
    "            else:\n",
    "                # Add to training set\n",
    "                train_file['raw'][train_idx, ...] = raw_dataset[i, ...]\n",
    "                train_file['label'][train_idx, ...] = label_dataset[i, ...]\n",
    "                train_idx += 1\n",
    "\n",
    "print(\"Training and validation HDF5 files have been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with h5py.File('sample_ovule_val.h5', 'r') as f:\n",
    "    print(f['raw'].shape)\n",
    "    print(f['label'].shape)\n",
    "\n",
    "with h5py.File('kidney_1_dense_train.hdf5', 'r') as f:\n",
    "    print(f['raw'].shape)\n",
    "    print(f['label'].shape)\n",
    "\n",
    "# # use PIL to show the first image from sample_ovule_val.h5\n",
    "# with h5py.File('sample_ovule_val.h5', 'r') as f:\n",
    "#     img = Image.fromarray(f['raw'][100, ...])\n",
    "#     img.show()\n",
    "\n",
    "\n",
    "# show the type of each channel in kidney_1_dense_train.hdf5\n",
    "with h5py.File('kidney_1_dense_train.hdf5', 'r') as f:\n",
    "    print(f['raw'].dtype)\n",
    "    print(f['label'].dtype)\n",
    "\n",
    "# show the label image from kidney_1_dense_train.hdf5\n",
    "with h5py.File('kidney_1_dense_train.hdf5', 'r') as f:\n",
    "    img = Image.fromarray(f['raw'][1000, ...])\n",
    "    img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get me the dimension of the kidney val dataset\n",
    "with h5py.File('kidney_1_dense_val.hdf5', 'r') as f:\n",
    "    print(f['raw'].shape)\n",
    "    print(f['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the size of the validation data\n",
    "with h5py.File('kidney_1_dense_val.hdf5', 'r') as f:\n",
    "    print(f['raw'].shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many images are in the training dataset\n",
    "with h5py.File('kidney_1_dense_train.hdf5', 'r') as f:\n",
    "    print(f['raw'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a grid of 8 images from the val dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "with h5py.File('kidney_1_dense_val.hdf5', 'r') as f:\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(8, 4))\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(f['raw'][i, ...], cmap='gray')\n",
    "        ax.set_title(f['label'][i, ...].max())\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config_xml = \"\"\"\n",
    "    # SliceBuilder configuration\n",
    "    slice_builder:\n",
    "      name: FilterSliceBuilder\n",
    "      # train patch size given to the network (adapt to fit in your GPU mem, generally the bigger patch the better)\n",
    "      patch_shape: [60, 200, 200]\n",
    "      # train stride between patches\n",
    "      stride_shape: [30, 100, 100]\n",
    "      # minimum volume of the labels in the patch\n",
    "      threshold: 0.01\n",
    "      # probability of accepting patches which do not fulfil the threshold criterion\n",
    "      slack_acceptance: 0.01\n",
    "     # data augmentation\n",
    "    transformer:\n",
    "      raw:\n",
    "        - name: Normalize\n",
    "          # parameters for the normalization\n",
    "          norm01: true\n",
    "        - name: ToTensor\n",
    "          expand_dims: true\n",
    "      label:\n",
    "        - name: Normalize\n",
    "          # parameters for the normalization\n",
    "          norm01: true\n",
    "        - name: ToTensor\n",
    "          expand_dims: true\n",
    "    \"\"\"\n",
    "\n",
    "import yaml\n",
    "# Parse the YAML configuration file\n",
    "test_config = yaml.safe_load(test_config_xml)\n",
    "\n",
    "# test the configuration\n",
    "test_config['slice_builder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import pytorch3dunet.datasets import FolderDataset from parent directory\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Assuming the 'pytorch3dunet' folder is in the parent directory of the notebook's directory\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "\n",
    "# Now you can import from pytorch3dunet\n",
    "from pytorch3dunet.datasets.folder import FolderDataset\n",
    "\n",
    "\n",
    "# Assuming FolderDataset is already defined and available\n",
    "# Define the paths to your images and labels\n",
    "image_path = 'kidney_1_dense/images'\n",
    "label_path = 'kidney_1_dense/labels'\n",
    "\n",
    "# Initialize the FolderDataset object\n",
    "dataset = FolderDataset(image_path, label_path, phase='train', slice_builder_config=test_config['slice_builder'], transformer_config=test_config['transformer'])\n",
    "\n",
    "def show_sample(image, label, index):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Display the first slice of the first channel of the image\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(image[0, 0, :, :], cmap='gray')  # First channel, first slice\n",
    "    plt.title(f'Image - Slice {index}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display the first slice of the first channel of the label\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(label[0, 0, :, :], cmap='gray')  # First channel, first slice\n",
    "    plt.title(f'Label - Slice {index}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "for i in range(500, 510, 1):\n",
    "    image, label = dataset[i]\n",
    "    print(f'Image shape: {image.shape}, Label shape: {label.shape}')\n",
    "    show_sample(image, label, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the datatype of\n",
    "\n",
    "kidney_2 = r'C:\\Users\\hamis\\OneDrive\\Documents\\GitHub\\human-vasculature-ml-comp\\blood-vessel-segmentation\\train\\kidney_1_dense'\n",
    "\n",
    "# check data type of images inside\n",
    "import os\n",
    "from glob import glob\n",
    "import tifffile\n",
    "\n",
    "image_dir = os.path.join(kidney_2, 'images')\n",
    "label_dir = os.path.join(kidney_2, 'labels')\n",
    "\n",
    "# Get sorted list of image and label paths\n",
    "image_paths = sorted(glob(os.path.join(image_dir, '*.tif')))\n",
    "\n",
    "# Read the first image to get datatype and dimensions\n",
    "sample_image = tifffile.imread(image_paths[0])\n",
    "\n",
    "print(sample_image.dtype)\n",
    "print(sample_image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_dataset = r'C:\\Users\\hamis\\OneDrive\\Documents\\GitHub\\human-vasculature-ml-comp\\blood-vessel-segmentation\\train\\kidney_2'\n",
    "\n",
    "## put every 4th image and label into a new folder called kidney_2_val' ##\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "image_dir = os.path.join(val_dataset, 'images')\n",
    "label_dir = os.path.join(val_dataset, 'labels')\n",
    "\n",
    "# Get sorted list of image and label paths\n",
    "image_paths = sorted(glob(os.path.join(image_dir, '*.tif')))\n",
    "label_paths = sorted(glob(os.path.join(label_dir, '*.tif')))\n",
    "\n",
    "# make a new folder called kidney_2_val\n",
    "os.mkdir(os.path.join(val_dataset, 'kidney_2_val'))\n",
    "\n",
    "# make a new folder called images inside kidney_2_val\n",
    "os.mkdir(os.path.join(val_dataset, 'kidney_2_val', 'images'))\n",
    "\n",
    "# make a new folder called labels inside kidney_2_val\n",
    "os.mkdir(os.path.join(val_dataset, 'kidney_2_val', 'labels'))\n",
    "\n",
    "# get the paths to the new folders\n",
    "new_image_dir = os.path.join(val_dataset, 'kidney_2_val', 'images')\n",
    "new_label_dir = os.path.join(val_dataset, 'kidney_2_val', 'labels')\n",
    "\n",
    "# copy every 4th image and label into the new folders\n",
    "for i in range(0, len(image_paths), 4):\n",
    "    os.rename(image_paths[i], os.path.join(new_image_dir, os.path.basename(image_paths[i])))\n",
    "    os.rename(label_paths[i], os.path.join(new_label_dir, os.path.basename(label_paths[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5505622659658369\n"
     ]
    }
   ],
   "source": [
    "import surface_distance\n",
    "import os\n",
    "from glob import glob\n",
    "import tifffile\n",
    "\n",
    "# get the first image and label from the kidney_1_dense to use as a test\n",
    "label = tifffile.imread(r'C:\\Users\\hamis\\OneDrive\\Documents\\GitHub\\human-vasculature-ml-comp\\blood-vessel-segmentation\\train\\kidney_1_dense\\labels\\1000.tif')\n",
    "label_2 = tifffile.imread(r'C:\\Users\\hamis\\OneDrive\\Documents\\GitHub\\human-vasculature-ml-comp\\blood-vessel-segmentation\\train\\kidney_1_dense\\labels\\1001.tif')\n",
    "\n",
    "# normalise to norm01\n",
    "label = label / label.max()\n",
    "label_2 = label_2 / label_2.max()\n",
    "\n",
    "# convert label to boolean\n",
    "label = label.astype(bool)\n",
    "label_2 = label_2.astype(bool)\n",
    "\n",
    "# check if any of the values in the label are true\n",
    "\n",
    "# get the surface distance\n",
    "surface_dist = surface_distance.compute_surface_distances(label, label_2, [1, 1])\n",
    "\n",
    "# compute surface dice\n",
    "surface_dice = surface_distance.compute_surface_dice_at_tolerance(surface_dist, 0)\n",
    "\n",
    "# print the surface dice\n",
    "print(surface_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5505622659658369"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch3dunet.unet3d.metrics import SurfaceDice\n",
    "# import tensor conversion\n",
    "import torch\n",
    "\n",
    "# create a surface distance object\n",
    "surface_dice = SurfaceDice(tolerance=0, spacing=[1,1])\n",
    "\n",
    "# compute the surface dice\n",
    "surface_dice(torch.from_numpy(label), torch.from_numpy(label_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
